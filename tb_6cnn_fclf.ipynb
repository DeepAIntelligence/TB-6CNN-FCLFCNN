{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepAIntelligence/TB-6CNN-FCLFCNN/blob/main/tb_6cnn_fclf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SylRsyAKfcXb"
      },
      "source": [
        "# Import Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iuj_AA1cCr7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as trs\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "from torchvision.transforms.transforms import Grayscale\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import PIL as pil\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchmetrics"
      ],
      "metadata": {
        "id": "DzTNMFFE1GQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "from torchmetrics.classification import MulticlassAUROC\n",
        "from torchmetrics.functional.classification import multiclass_auroc\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall"
      ],
      "metadata": {
        "id": "-ZQ3Jkyw1Tlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect Google Colab To Google Drive"
      ],
      "metadata": {
        "id": "snD9TpWm3jMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzV4ZzCoaGHL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GPU"
      ],
      "metadata": {
        "id": "NkuEg3703qJd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5VaP-j7c_-T"
      },
      "outputs": [],
      "source": [
        "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ww8ALGAmGjz"
      },
      "source": [
        "################################################################################\n",
        "# Load Data\n",
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuJzqtrVE0AC"
      },
      "source": [
        "## Make Train,Val,Test Sets by ImagesNameList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSef7-J9E5Nj"
      },
      "outputs": [],
      "source": [
        "Book=pd.read_csv('/content/drive/MyDrive/ImagesList.csv')\n",
        "\n",
        "tb_Book=Book[:800]\n",
        "health_Book=Book[800:4600]\n",
        "sick_Book=Book[4600:8400]\n",
        "\n",
        "tb_Book = tb_Book.sample(frac=1,random_state=1).reset_index(drop=True)\n",
        "health_Book = health_Book.sample(frac=1,random_state=1).reset_index(drop=True)\n",
        "sick_Book = sick_Book.sample(frac=1,random_state=1).reset_index(drop=True)\n",
        "\n",
        "tb_train=tb_Book[:396]\n",
        "tb_val=tb_Book[396:524]\n",
        "tb_test=tb_Book[524:800]\n",
        "health_train=health_Book[:2280]\n",
        "health_val=health_Book[2280:2888]\n",
        "health_test=health_Book[2888:3800]\n",
        "sick_train=sick_Book[:2280]\n",
        "sick_val=sick_Book[2280:2888]\n",
        "sick_test=sick_Book[2888:3800]\n",
        "\n",
        "train_Book=pd.concat([tb_train,health_train,sick_train])\n",
        "val_Book=pd.concat([tb_val,health_val,sick_val])\n",
        "test_Book=pd.concat([tb_test,health_test,sick_test])\n",
        "\n",
        "train_Book = train_Book.sample(frac=1,random_state=1).reset_index(drop=True)\n",
        "val_Book = val_Book.sample(frac=1,random_state=1).reset_index(drop=True)\n",
        "test_Book = test_Book.sample(frac=1,random_state=1).reset_index(drop=True)\n",
        "\n",
        "# test_Book = test_Book [0:150]\n",
        "\n",
        "print(train_Book)\n",
        "print(val_Book)\n",
        "print(test_Book)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li9IJ8KPaC0f"
      },
      "source": [
        "## Define a Class of Custom DataLoader To Recieve and PreProcess Image by Name "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "473wyMyAaHCQ"
      },
      "outputs": [],
      "source": [
        "class custom_data (data.Dataset):\n",
        "  def __init__(self,root,book,size):\n",
        "    self.book = book\n",
        "    self.size = size\n",
        "    self.root = root\n",
        "    self.transform=trs.Compose([trs.ToPILImage(),\n",
        "                                trs.Resize(256),\n",
        "                                trs.ToTensor()])\n",
        "\n",
        "  def _get(self):\n",
        "    lst_img=[]\n",
        "    images_lable=[]\n",
        "    for idx in range(0,self.size):\n",
        "      # print(idx)\n",
        "      image_name=self.book.iloc[idx,0]\n",
        "      image_class=self.book.iloc[idx,1]\n",
        "      image_lable=self.book.iloc[idx,2]\n",
        "      img=cv2.imread('%s/%s/%s.png'%(self.root, image_class,image_name))\n",
        "      img=torch.tensor(img)\n",
        "      img=img.permute(2,0,1)\n",
        "      img=img.float()\n",
        "      img=self.transform(img)\n",
        "      img=torch.tensor(img,device=dev)\n",
        "      image_lable=torch.tensor(image_lable,device=dev)\n",
        "      imge=(img,image_lable)\n",
        "      lst_img.append(imge)\n",
        "      images_lable.append(image_lable)\n",
        "\n",
        "    return lst_img ,images_lable\n",
        "\n",
        "  def _remove(self):\n",
        "    size=range(0,self.size)\n",
        "    book = self.book.drop(index=size).reset_index(drop=True)\n",
        "    return book"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j78gTtzRzg53"
      },
      "source": [
        "## Unzip Images To Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-vzpIUozn2A"
      },
      "outputs": [],
      "source": [
        "! unzip /content/drive/MyDrive/health.zip\n",
        "! unzip /content/drive/MyDrive/sick.zip\n",
        "! unzip /content/drive/MyDrive/tb.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCkLEWsGmsyV"
      },
      "source": [
        "################################################################################\n",
        "# Define Models\n",
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_GNx-beBhKm"
      },
      "source": [
        "## InceptionV3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOTK8g8gBhKm"
      },
      "outputs": [],
      "source": [
        "class convnet_inceptionv3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(convnet_inceptionv3,self).__init__()\n",
        "    self.base = models.Inception3(aux_logits=False)\n",
        "    self.base.fc=nn.Linear(in_features=2048,out_features=3,bias=True)\n",
        "    # self.base.aux_logits=False\n",
        "\n",
        "  def forward(self,x):\n",
        "    y=self.base(x.clone())\n",
        "    return y\n",
        "\n",
        "model_inceptionv3 = convnet_inceptionv3().to(dev)\n",
        "print(model_inceptionv3)\n",
        "summary(model_inceptionv3,(3,256,256))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYCT4vAdBhKs"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-bHOoKMBhKs"
      },
      "outputs": [],
      "source": [
        "class convnet_res50(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(convnet_res50,self).__init__()\n",
        "    self.base = models.resnet50(pretrained=True)\n",
        "    self.base.fc = nn.Linear(in_features=2048 ,out_features=3,bias=True)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    y=self.base(x.clone())\n",
        "    return y\n",
        "\n",
        "model_res50 = convnet_res50().to(dev)\n",
        "print(model_res50)\n",
        "summary(model_res50,(3,256,256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGJG5qaIBhK7"
      },
      "source": [
        "## DenseNet201"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCTvB0EIBhK8"
      },
      "outputs": [],
      "source": [
        "class convnet_dense201(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(convnet_dense201,self).__init__()\n",
        "    self.base = models.densenet201(pretrained=True)\n",
        "    self.base.classifier = nn.Linear(in_features=1920 ,out_features=3,bias=True)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    y=self.base(x.clone())\n",
        "    return y\n",
        "\n",
        "model_dense201 = convnet_dense201().to(dev)\n",
        "print(model_dense201)\n",
        "\n",
        "model_parameters = filter(lambda p: p.requires_grad, model_dense201.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jEvLgjsBhLB"
      },
      "source": [
        "## MnasNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAkg0cwnBhLE"
      },
      "outputs": [],
      "source": [
        "class convnet_mnas(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(convnet_mnas,self).__init__()\n",
        "    self.base = models.mnasnet1_0(pretrained=True)\n",
        "    self.base.layers[1]=nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[4]=nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[7]=nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[8][0].layers[1]=nn.BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[8][0].layers[4]=nn.BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[8][0].layers[7]=nn.BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[8][1].layers[1]=nn.BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[8][1].layers[4]=nn.BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[8][1].layers[7]=nn.BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[8][2].layers[1]=nn.BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[8][2].layers[4]=nn.BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[8][2].layers[7]=nn.BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[9][0].layers[1]=nn.BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[9][0].layers[4]=nn.BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[9][0].layers[7]=nn.BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[9][1].layers[1]=nn.BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[9][1].layers[4]=nn.BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[9][1].layers[7]=nn.BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[9][2].layers[1]=nn.BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[9][2].layers[4]=nn.BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[9][2].layers[7]=nn.BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[10][0].layers[1]=nn.BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[10][0].layers[4]=nn.BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[10][0].layers[7]=nn.BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[10][1].layers[1]=nn.BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[10][1].layers[4]=nn.BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[10][1].layers[7]=nn.BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[10][2].layers[1]=nn.BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[10][2].layers[4]=nn.BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[10][2].layers[7]=nn.BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[11][0].layers[1]=nn.BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[11][0].layers[4]=nn.BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[11][0].layers[7]=nn.BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[11][1].layers[1]=nn.BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[11][1].layers[4]=nn.BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[11][1].layers[7]=nn.BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][0].layers[1]=nn.BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][0].layers[4]=nn.BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][0].layers[7]=nn.BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][1].layers[1]=nn.BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][1].layers[4]=nn.BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][1].layers[7]=nn.BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][2].layers[1]=nn.BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][2].layers[4]=nn.BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][2].layers[7]=nn.BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][3].layers[1]=nn.BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][3].layers[4]=nn.BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[12][3].layers[7]=nn.BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[13][0].layers[1]=nn.BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[13][0].layers[4]=nn.BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[13][0].layers[7]=nn.BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.layers[15]=nn.BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "    self.base.classifier[1] = nn.Linear(in_features=1280 ,out_features=3,bias=True)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    y=self.base(x.clone())\n",
        "    return y\n",
        "\n",
        "model_mnas = convnet_mnas().to(dev)\n",
        "print(model_mnas)\n",
        "summary(model_mnas,(3,256,256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylbg6eKiBhKh"
      },
      "source": [
        "## MobilenetV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMkSttoPBhKi"
      },
      "outputs": [],
      "source": [
        "class convnet_mobilev3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(convnet_mobilev3,self).__init__()\n",
        "    self.base = models.mobilenet_v3_large(pretrained=True)\n",
        "\n",
        "    self.base.features[0][1]=nn.BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[1].block[0][1]=nn.BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[1].block[1][1]=nn.BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[2].block[0][1]=nn.BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[2].block[1][1]=nn.BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[2].block[2][1]=nn.BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[3].block[0][1]=nn.BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[3].block[1][1]=nn.BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[3].block[2][1]=nn.BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[4].block[0][1]=nn.BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[4].block[1][1]=nn.BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[4].block[3][1]=nn.BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[5].block[0][1]=nn.BatchNorm2d(120, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[5].block[1][1]=nn.BatchNorm2d(120, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[5].block[3][1]=nn.BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[6].block[0][1]=nn.BatchNorm2d(120, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[6].block[1][1]=nn.BatchNorm2d(120, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[6].block[3][1]=nn.BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[7].block[0][1]=nn.BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[7].block[1][1]=nn.BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[7].block[2][1]=nn.BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[8].block[0][1]=nn.BatchNorm2d(200, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[8].block[1][1]=nn.BatchNorm2d(200, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[8].block[2][1]=nn.BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[9].block[0][1]=nn.BatchNorm2d(184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[9].block[1][1]=nn.BatchNorm2d(184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[9].block[2][1]=nn.BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[10].block[0][1]=nn.BatchNorm2d(184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[10].block[1][1]=nn.BatchNorm2d(184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[10].block[2][1]=nn.BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[11].block[0][1]=nn.BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[11].block[1][1]=nn.BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[11].block[3][1]=nn.BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[12].block[0][1]=nn.BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[12].block[1][1]=nn.BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[12].block[3][1]=nn.BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[13].block[0][1]=nn.BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[13].block[1][1]=nn.BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[13].block[3][1]=nn.BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[14].block[0][1]=nn.BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[14].block[1][1]=nn.BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[14].block[3][1]=nn.BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[15].block[0][1]=nn.BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[15].block[1][1]=nn.BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[15].block[3][1]=nn.BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    self.base.features[16][1]=nn.BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "    self.base.classifier[3]=nn.Linear(in_features=1280,out_features=3,bias=True)\n",
        "  def forward(self,x):\n",
        "    y=self.base(x.clone())\n",
        "    return y\n",
        "\n",
        "model_mobilev3 = convnet_mobilev3().to(dev)\n",
        "print(model_mobilev3)\n",
        "summary(model_mobilev3,(3,256,256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OUgvN2ZBhLG"
      },
      "source": [
        "## EfficientNet-B4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJe_OsZkBhLG"
      },
      "outputs": [],
      "source": [
        "class convnet_effi_b4(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(convnet_effi_b4,self).__init__()\n",
        "    self.base = models.efficientnet_b4(pretrained=True)\n",
        "    self.base.classifier[1] = nn.Linear(in_features=1792 ,out_features=3,bias=True)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    y=self.base(x.clone())\n",
        "    return y\n",
        "\n",
        "model_effi_b4 = convnet_effi_b4().to(dev)\n",
        "print(model_effi_b4)\n",
        "summary(model_effi_b4,(3,256,256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyr028SbBhLH"
      },
      "source": [
        "## Fully Connected Layer First\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnEnKqyNBhLI"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "class NeuralNetwork_f(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork_f, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(3*256*256, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 3*256*256)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        y = self.linear_relu_stack(x)\n",
        "        return y,x\n",
        "\n",
        "model_f = NeuralNetwork_f().to(dev)\n",
        "print(model_f)\n",
        "summary(model_f,(3,256,256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8sdU22qKYjf"
      },
      "source": [
        "################################################################################\n",
        "# Optimizing the Model Parameters\n",
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The optimizer for the defined networks should be run**\n",
        "\n"
      ],
      "metadata": {
        "id": "kPn934BVcv-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5rwozQaKYjf"
      },
      "outputs": [],
      "source": [
        "loss_fn_mse = nn.MSELoss()\n",
        "loss_fn_ce = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_inceptionv3 = torch.optim.Adam(model_inceptionv3.parameters(), lr=0.001)\n",
        "optimizer_res50 = torch.optim.Adam(model_res50.parameters(), lr=0.001)\n",
        "optimizer_res18 = torch.optim.Adam(model_res18.parameters(), lr=0.001)\n",
        "optimizer_dense201 = torch.optim.Adam(model_dense201.parameters(), lr=0.001)\n",
        "optimizer_mnas = torch.optim.Adam(model_mnas.parameters(), lr=0.001)\n",
        "optimizer_mobilev3 = torch.optim.Adam(model_mobilev3.parameters(), lr=0.001)\n",
        "optimizer_effi_b4 = torch.optim.Adam(model_effi_b4.parameters(), lr=0.001)\n",
        "optimizer_f = torch.optim.Adam(model_f.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vHGi13btaGR"
      },
      "source": [
        "################################################################################\n",
        "# Define Functions To Run\n",
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4hRm6Lxaf8I"
      },
      "source": [
        "## Function To Training CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDiLvm5Ug1rE"
      },
      "outputs": [],
      "source": [
        "def train_func(dataset, model, loss, optimizer, batch_size, epoch):\n",
        "  vol=len(dataset)\n",
        "  iter_s = vol // batch_size\n",
        "  iter_f = vol % batch_size\n",
        "  \n",
        "  number_batch=0\n",
        "  \n",
        "  loss_all=[]\n",
        "  yd_all=[]\n",
        "  y_all=[]\n",
        "\n",
        "  for idx in range(0,iter_s):\n",
        "    number_batch=int(idx)+1\n",
        "    print('epoch=' ,epoch ,'batch = ',number_batch)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=batch_size)\n",
        "    images,lables=batch_images._get()\n",
        "\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=batch_size)\n",
        "\n",
        "    for (x,y) in data_loader:\n",
        "      model.train(True)\n",
        "      # Compute prediction error\n",
        "      yp=model(x)\n",
        "\n",
        "      loss_value = loss(yp,y)\n",
        "\n",
        "      yd=(torch.max(yp,1)[1])\n",
        "      print('(',yd,')','|' , '(',y,')')\n",
        "\n",
        "      num_corrects=torch.sum(torch.max(yp,1)[1]==y)\n",
        "      acc = num_corrects.float()/float(batch_size)\n",
        "\n",
        "      my_loss=loss_value\n",
        "      my_yd=yd\n",
        "      my_y=y\n",
        "      loss_all.append(my_loss.detach().cpu().numpy().item())\n",
        "      yd_all.append(my_yd.cpu().numpy())\n",
        "      y_all.append(my_y.cpu().numpy())\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss_value.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  \n",
        "      print('num_corrects:' , num_corrects.item(),',   Train Loss:', loss_value.item(),\n",
        "             ',   Train Accuracy:' , acc.item())\n",
        "########################################################################################################\n",
        "  if iter_f > 0 :\n",
        "    print('epoch=' ,epoch ,'last batch = ' ,number_batch+1)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=iter_f)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=iter_f)\n",
        "\n",
        "    for (x,y) in data_loader:\n",
        "      # Compute prediction error\n",
        "      model.train(True)\n",
        "      yp=model(x)\n",
        "\n",
        "      loss_value = loss(yp,y)\n",
        "      yd=(torch.max(yp,1)[1])\n",
        "      print('(',yd,')','|' , '(',y,')')\n",
        "\n",
        "      num_corrects=torch.sum(torch.max(yp,1)[1]==y)\n",
        "      acc = num_corrects.float()/float(iter_f)\n",
        "\n",
        "      my_loss=loss_value\n",
        "      my_yd=yd\n",
        "      my_y=y\n",
        "      loss_all.append(my_loss.detach().cpu().numpy().item())\n",
        "      yd_all.append(my_yd.cpu().numpy())\n",
        "      y_all.append(my_y.cpu().numpy())\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss_value.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      print('num_corrects:' , num_corrects.item(),',   Train Loss:', loss_value.item(),\n",
        "              ',   Train Accuracy:' , acc.item())\n",
        "      \n",
        "  return loss_all , yd_all , y_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lWaYuWLgm9c"
      },
      "source": [
        "## Function To Validation and Testing CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8ZOjDxCgm91"
      },
      "outputs": [],
      "source": [
        "def test_func(dataset, model, loss, batch_size, epoch):\n",
        "\n",
        "  vol=len(dataset)\n",
        "  iter_s = vol // batch_size\n",
        "  iter_f = vol % batch_size\n",
        "  \n",
        "  number_batch=0\n",
        "  \n",
        "  loss_all=[]\n",
        "  yd_all=[]\n",
        "  y_all=[]\n",
        "  yp_all=torch.zeros(0,3).to(dev)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  loss_value=0\n",
        "  for idx in range(0,iter_s):\n",
        "    number_batch=int(idx)+1\n",
        "    print('epoch=' ,epoch ,'batch = ',number_batch)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=batch_size)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "     for (x,y) in data_loader:\n",
        "      model.train(True)\n",
        "      # Compute prediction error\n",
        "      yp=model(x)\n",
        "      loss_value = loss(yp,y)\n",
        "\n",
        "      yd=(torch.max(yp,1)[1])\n",
        "      print('(',yd,')','|' , '(',y,')')\n",
        "\n",
        "      num_corrects=torch.sum(torch.max(yp,1)[1]==y)\n",
        "      acc = num_corrects.float()/float(batch_size)\n",
        "\n",
        "      my_loss=loss_value\n",
        "      my_yd=yd\n",
        "      my_y=y\n",
        "      loss_all.append(my_loss.detach().cpu().numpy().item())\n",
        "      yd_all.append(my_yd.cpu().numpy())\n",
        "      y_all.append(my_y.cpu().numpy())\n",
        "      yp.to(dev)\n",
        "      yp_all= torch.cat((yp_all, yp), 0)\n",
        "\n",
        "      print('num_corrects:' , num_corrects.item(),',   val Loss:', loss_value.item(),\n",
        "             ',   val Accuracy:' , acc.item())\n",
        "########################################################################################################\n",
        "  if iter_f > 0 :\n",
        "    print('epoch=' ,epoch ,'last batch = ' ,number_batch+1)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=iter_f)\n",
        "    images,lables=batch_images._get()\n",
        "    print('img=',torch.cuda.memory_allocated(device=dev))\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=iter_f)\n",
        "\n",
        "    with torch.no_grad():\n",
        "     for (x,y) in data_loader:\n",
        "      model.train(True)\n",
        "\n",
        "      # Compute prediction error\n",
        "      yp=model(x)\n",
        "      loss_value = loss(yp,y)\n",
        "      yd=(torch.max(yp,1)[1])\n",
        "      print('(',yd,')','|' , '(',y,')')\n",
        "\n",
        "      num_corrects=torch.sum(torch.max(yp,1)[1]==y)\n",
        "      acc = num_corrects.float()/float(iter_f)\n",
        "\n",
        "      my_loss=loss_value\n",
        "      my_yd=yd\n",
        "      my_y=y\n",
        "      loss_all.append(my_loss.detach().cpu().numpy().item())\n",
        "      yd_all.append(my_yd.cpu().numpy())\n",
        "      y_all.append(my_y.cpu().numpy())\n",
        "      yp.to(dev)\n",
        "      yp_all= torch.cat((yp_all, yp), 0)\n",
        "\n",
        "      print('num_corrects:' , num_corrects.item(),',   val Loss:', loss_value.item(),\n",
        "              ',   val Accuracy:' , acc.item())\n",
        "      \n",
        "  return loss_all , yd_all , y_all , yp_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YweHR3oW6zfx"
      },
      "source": [
        "## Function To Run CNN ,Save Models And Train&Val Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emivJfqmG11i"
      },
      "outputs": [],
      "source": [
        "def run_func (name , model_fn , loss_fn , optimizer_fn , epoch , batch):\n",
        "\n",
        "  tr_loss , tr_yd , tr_y =[],[],[]\n",
        "  val_loss , val_yd , val_y =[],[],[]\n",
        "  test_loss , test_yd , test_y =[],[],[]\n",
        "\n",
        "  # train and val\n",
        "  for epo in range(0,epoch):\n",
        "    print(' ')\n",
        "    print('epoch ========================================================================================= ',epo+1)\n",
        "    print(' ')\n",
        "    print('trainnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn')\n",
        "    print(' ')\n",
        "\n",
        "    tr_loss_e , tr_yd_e , tr_y_e = train_func(dataset=train_Book ,model=model_fn , loss=loss_fn ,optimizer=optimizer_fn ,batch_size=batch ,epoch=epo+1)\n",
        "    print(' ')\n",
        "    torch.save(model_fn.state_dict(), '/content/drive/MyDrive/modelsaved/saved_model_'+name+'_epoch='+str(epo+1)+'.pth')\n",
        "\n",
        "    print('valllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll')\n",
        "    print(' ')\n",
        "\n",
        "    val_loss_e , val_yd_e , val_y_e = test_func(dataset=val_Book ,model=model_fn , loss=loss_fn ,batch_size=batch,epoch=epo+1)\n",
        "\n",
        "    tr_loss.append(tr_loss_e)\n",
        "    tr_yd.append(tr_yd_e)\n",
        "    tr_y.append(tr_y_e)\n",
        "    val_loss.append(val_loss_e)\n",
        "    val_yd.append(val_yd_e)\n",
        "    val_y.append(val_y_e)\n",
        "\n",
        "  np.savez('/content/drive/MyDrive/train_result_'+name   , tr_loss , tr_yd , tr_y)\n",
        "  np.savez('/content/drive/MyDrive/val_result_'+name     , val_loss , val_yd , val_y)\n",
        "  \n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S-X1QENAC6n"
      },
      "source": [
        "## Function To Load Models Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCCAOCNYAHUX"
      },
      "outputs": [],
      "source": [
        "def load_model (model,name , epoch):\n",
        "\n",
        "  model.load_state_dict(torch.load('/content/drive/MyDrive/modelsaved/saved_model_'+name+'_epoch='+str(epoch)+'.pth'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zaktL-AM68C"
      },
      "source": [
        "## Function To Load Train&Val Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPR63EYr0fMs"
      },
      "outputs": [],
      "source": [
        "def func_load (name):\n",
        "  train=np.load('/content/drive/MyDrive/train_result_'+name+'.npz',allow_pickle=True)\n",
        "  val=np.load('/content/drive/MyDrive/val_result_'+name+'.npz',allow_pickle=True)\n",
        "\n",
        "  sorted(train.files)\n",
        "  a1=train['arr_0']\n",
        "  a2=train['arr_1']\n",
        "  a3=train['arr_2']\n",
        "\n",
        "  sorted(val.files)\n",
        "  b1=val['arr_0']\n",
        "  b2=val['arr_1']\n",
        "  b3=val['arr_2']\n",
        "\n",
        "  all_result=[[a1,a2,a3],[b1,b2,b3]]\n",
        "  return all_result \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G54hPgTCBA00"
      },
      "source": [
        "## Function To plot Accuracy And Loss of Train&Val Results  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggq2P_64BA03"
      },
      "outputs": [],
      "source": [
        "def acc_mat_func (doc,number_train_img,numbatch_fortrain_loss,number_val_img,numbatch_forval_loss):\n",
        "  name = func_load(doc)\n",
        "  lst=name\n",
        "  train=lst[0]\n",
        "  val=lst[1]\n",
        "\n",
        "  number_train_loss_epoch=len(train[0][0])\n",
        "  number_train_epoch=len(train[1])\n",
        "  number_train_batch=len(train[1][0])-1\n",
        "  number_train_batchsize=len(train[1][0][0])\n",
        "  number_train_endbatch= 4956 % number_train_batchsize\n",
        "  if (number_train_endbatch==0):\n",
        "    number_train_endbatch=16\n",
        "  print('number_train_loss_epoch',number_train_loss_epoch)\n",
        "  print('number_train_epoch',number_train_epoch)\n",
        "  print('number_train_batch',number_train_batch)\n",
        "  print('number_train_batchsize',number_train_batchsize)\n",
        "  print('number_train_endbatch',number_train_endbatch)\n",
        "  print('shape',train[0].shape)\n",
        "  number_val_loss_epoch=len(val[0][0])\n",
        "  number_val_epoch=len(val[1])\n",
        "  number_val_batch=len(val[1][0])-1\n",
        "  number_val_batchsize=len(val[1][0][0])\n",
        "  number_val_endbatch= 1344 % number_val_batchsize\n",
        "  if (number_val_endbatch==0):\n",
        "    number_val_endbatch=16\n",
        "  print('number_val_loss_epoch',number_val_loss_epoch)\n",
        "  print('number_val_epoch',number_val_epoch)\n",
        "  print('number_val_batch',number_val_batch)\n",
        "  print('number_val_batchsize',number_val_batchsize)\n",
        "  print('number_val_endbatch',number_val_endbatch)\n",
        "\n",
        "#########################################################################     train\n",
        "  print('################################################################     train     ###################################################################')\n",
        "\n",
        "  train_loss=train[0]\n",
        "  train_yd=[]\n",
        "  train_y=[]\n",
        "\n",
        "  batch_fix=0\n",
        "  for epoch in range(0,number_train_epoch):\n",
        "    for batch in range(0,number_train_batch):\n",
        "      \n",
        "      batch_fix=batch+1\n",
        "      for size in range(0,number_train_batchsize):\n",
        "        train_yd.append(train[1][epoch][batch][size])\n",
        "        train_y.append(train[2][epoch][batch][size])\n",
        "\n",
        "    for size in range(0,number_train_endbatch):\n",
        "      train_yd.append(train[1][epoch][batch_fix][size])\n",
        "      train_y.append(train[2][epoch][batch_fix][size])\n",
        "\n",
        "  print('len(train_loss)',len(train_loss))\n",
        "  print('len(train_yd)',len(train_yd))\n",
        "  print('len(train_y)',len(train_y))\n",
        "\n",
        "  train_acc_list=[]\n",
        "  \n",
        "  number_a_epoch=int(len(train_yd)/number_train_epoch)\n",
        "  number_train_point=number_a_epoch // number_train_img\n",
        "  number_train_end=int(len(train_yd)/number_train_epoch) % number_train_img\n",
        "  print('number_a_epoch_train',number_a_epoch)\n",
        "  print('number_train_epoch',number_train_epoch)\n",
        "  print('number_train_point',number_train_point)\n",
        "  print('number_train_end',number_train_end)\n",
        "\n",
        "  for epoch in range(0,number_train_epoch):\n",
        "    print('epoch',epoch)\n",
        "    for point in range(0,int(number_train_point)):\n",
        "      num_train=0\n",
        "      for id in range(0,number_train_img):\n",
        "        train_yd_b=torch.tensor(train_yd[(epoch*number_a_epoch)+(point*number_train_img)+id])\n",
        "        train_y_b=torch.tensor(train_y[(epoch*number_a_epoch)+(point*number_train_img)+id])\n",
        "        num_corrects_train=torch.sum(train_yd_b==train_y_b)\n",
        "        num_train+=num_corrects_train\n",
        "      acc_train=num_train/number_train_img*100\n",
        "      print('num_train',num_train,'    number_train_img',number_train_img)\n",
        "\n",
        "      print('acc_train',acc_train)\n",
        "      train_acc_list.append(acc_train)\n",
        "      print('train_acc_list.shape',len(train_acc_list))\n",
        "\n",
        "    num_train=0\n",
        "    if (number_train_end>0):\n",
        "      for end in range (0,number_train_end):\n",
        "        train_yd_b=torch.tensor(train_yd[(epoch*number_train_epoch)+(int(number_train_point)*number_train_img)+end])\n",
        "        train_y_b=torch.tensor(train_y[(epoch*number_train_epoch)+(int(number_train_point)*number_train_img)+end])\n",
        "        num_corrects_train=torch.sum(train_yd_b==train_y_b)\n",
        "        num_train+=num_corrects_train\n",
        "      acc_train=num_train/number_train_end*100\n",
        "      print('num_train',num_train,'    number_train_end',number_train_end)\n",
        "      print('acc_train',acc_train)\n",
        "      train_acc_list.append(acc_train)\n",
        "      print('train_acc_list.shape',len(train_acc_list))\n",
        "\n",
        "    num_train=0\n",
        "\n",
        "  train_loss_mean = []\n",
        "  numloop_for_loss=int(number_train_loss_epoch // numbatch_fortrain_loss)\n",
        "  endloop_for_loss=int(number_train_loss_epoch % numbatch_fortrain_loss)\n",
        "  print('numbatch_fortrain_loss',numbatch_fortrain_loss)\n",
        "  print('numloop_for_loss',numloop_for_loss)\n",
        "  print('endloop_for_loss',endloop_for_loss)\n",
        "\n",
        "  sumy=0\n",
        "  mean=0  \n",
        "\n",
        "  if (endloop_for_loss==0):\n",
        "   for epoch in range(0,number_train_epoch):\n",
        "    for loop in range(0, numloop_for_loss):\n",
        "    \n",
        "      for inloop in range(0,numbatch_fortrain_loss-1):\n",
        "        losses_train = train_loss[epoch][(loop*numbatch_fortrain_loss)+inloop]*number_train_batchsize\n",
        "        sumy = sumy + losses_train\n",
        "      losses_train = train_loss[epoch][(loop*numbatch_fortrain_loss)+inloop+1]*number_train_endbatch\n",
        "      sumy = sumy + losses_train\n",
        "      mean = sumy / (((numbatch_fortrain_loss-1)*number_train_batchsize)+(number_train_endbatch))\n",
        "      train_loss_mean.append(mean)\n",
        "      sumy=0\n",
        "      mean=0\n",
        "\n",
        "  elif (endloop_for_loss>0):\n",
        "   for epoch in range(0,number_train_epoch):\n",
        "    for loop in range(0, numloop_for_loss):\n",
        "      for inloop in range(0,numbatch_fortrain_loss):\n",
        "        losses_train = train_loss[epoch][(loop*numbatch_fortrain_loss)+inloop]*number_train_batchsize\n",
        "        sumy = sumy + losses_train\n",
        "      mean = sumy / (numbatch_fortrain_loss*number_train_batchsize)\n",
        "      train_loss_mean.append(mean)\n",
        "      sumy=0\n",
        "      mean=0\n",
        "    loop+=1\n",
        "    for endloop in range(0,endloop_for_loss-1):\n",
        "      losses_train = train_loss[epoch][(loop*numbatch_fortrain_loss)+endloop]*number_train_batchsize\n",
        "      sumy = sumy + losses_train\n",
        "    losses_train = train_loss[epoch][(loop*numbatch_fortrain_loss)+endloop+1]*number_train_endbatch\n",
        "    sumy = sumy + losses_train\n",
        "    mean = sumy / (((numbatch_fortrain_loss-1)*number_train_batchsize)+(number_train_endbatch))\n",
        "    train_loss_mean.append(mean)\n",
        "    sumy=0\n",
        "    mean=0\n",
        "    \n",
        "#########################################################################     val\n",
        "  print('################################################################     val     ###################################################################')\n",
        "\n",
        "  val_loss=val[0]\n",
        "  val_yd=[]\n",
        "  val_y=[]\n",
        "\n",
        "  batch_fix=0\n",
        "  for epoch in range(0,number_val_epoch):\n",
        "    for batch in range(0,number_val_batch):\n",
        "      \n",
        "      batch_fix=batch+1\n",
        "      for size in range(0,number_val_batchsize):\n",
        "        val_yd.append(val[1][epoch][batch][size])\n",
        "        val_y.append(val[2][epoch][batch][size])\n",
        "\n",
        "    for size in range(0,number_val_endbatch):\n",
        "      val_yd.append(val[1][epoch][batch_fix][size])\n",
        "      val_y.append(val[2][epoch][batch_fix][size])\n",
        "\n",
        "  print('len(val_loss)',len(val_loss))\n",
        "  print('len(val_yd)',len(val_yd))\n",
        "  print('len(val_y)',len(val_y))\n",
        "\n",
        "  val_acc_list=[]\n",
        "  \n",
        "  number_a_epoch=int(len(val_yd)/number_val_epoch)\n",
        "  number_val_point=number_a_epoch // number_val_img\n",
        "  number_val_end=int(len(val_yd)/number_val_epoch) % number_val_img\n",
        "  print('number_a_epoch_val',number_a_epoch)\n",
        "  print('number_val_epoch',number_val_epoch)\n",
        "  print('number_val_point',number_val_point)\n",
        "  print('number_val_end',number_val_end)\n",
        "\n",
        "  for epoch in range(0,number_val_epoch):\n",
        "    print('epoch',epoch)\n",
        "    for point in range(0,int(number_val_point)):\n",
        "      num_val=0\n",
        "      for id in range(0,number_val_img):\n",
        "        val_yd_b=torch.tensor(val_yd[(epoch*number_a_epoch)+(point*number_val_img)+id])\n",
        "        val_y_b=torch.tensor(val_y[(epoch*number_a_epoch)+(point*number_val_img)+id])\n",
        "        num_corrects_val=torch.sum(val_yd_b==val_y_b)\n",
        "        num_val+=num_corrects_val\n",
        "\n",
        "      acc_val=num_val/number_val_img*100\n",
        "      print('acc_val',acc_val)\n",
        "      val_acc_list.append(acc_val)\n",
        "\n",
        "\n",
        "    num_val=0\n",
        "    if (number_val_end>0):\n",
        "      for end in range (0,number_val_end):\n",
        "        val_yd_b=torch.tensor(val_yd[(epoch*number_val_epoch)+(int(number_val_point)*number_val_img)+end])\n",
        "        val_y_b=torch.tensor(val_y[(epoch*number_val_epoch)+(int(number_val_point)*number_val_img)+end])\n",
        "        num_corrects_val=torch.sum(val_yd_b==val_y_b)\n",
        "        num_val+=num_corrects_val\n",
        "      acc_val=num_val/number_val_end*100\n",
        "      print('acc_val',acc_val)\n",
        "      val_acc_list.append(acc_val)\n",
        "    num_val=0\n",
        "\n",
        "\n",
        "  val_loss_mean = []\n",
        "  numloop_for_loss=int(number_val_loss_epoch // numbatch_forval_loss)\n",
        "  endloop_for_loss=int(number_val_loss_epoch % numbatch_forval_loss)\n",
        "  print('numbatch_forval_loss',numbatch_forval_loss)\n",
        "  print('numloop_for_loss',numloop_for_loss)\n",
        "  print('endloop_for_loss',endloop_for_loss)\n",
        "\n",
        "  sumy=0\n",
        "  mean=0  \n",
        "\n",
        "  if (endloop_for_loss==0):\n",
        "   for epoch in range(0,number_val_epoch):\n",
        "    for loop in range(0, numloop_for_loss):\n",
        "    \n",
        "      for inloop in range(0,numbatch_forval_loss-1):\n",
        "        losses_val = val_loss[epoch][(loop*numbatch_forval_loss)+inloop]*number_val_batchsize\n",
        "        sumy = sumy + losses_val\n",
        "      losses_val = val_loss[epoch][(loop*numbatch_forval_loss)+inloop+1]*number_val_endbatch\n",
        "      sumy = sumy + losses_val\n",
        "      mean = sumy / (((numbatch_forval_loss-1)*number_val_batchsize)+(number_val_endbatch))\n",
        "      val_loss_mean.append(mean)\n",
        "      print('  mean=',mean , '  val_loss_mean.shape',len(val_loss_mean))\n",
        "      sumy=0\n",
        "      mean=0\n",
        "\n",
        "  elif (endloop_for_loss>0):\n",
        "   for epoch in range(0,number_val_epoch):\n",
        "    for loop in range(0, numloop_for_loss):\n",
        "      for inloop in range(0,numbatch_forval_loss):\n",
        "        losses_val = val_loss[epoch][(loop*numbatch_forval_loss)+inloop]*number_val_batchsize\n",
        "        sumy = sumy + losses_val\n",
        "      mean = sumy / (numbatch_forval_loss*number_val_batchsize)\n",
        "      val_loss_mean.append(mean)\n",
        "      print('  mean=',mean , '  val_loss_mean.shape',len(val_loss_mean))\n",
        "      sumy=0\n",
        "      mean=0\n",
        "    loop+=1\n",
        "    for endloop in range(0,endloop_for_loss-1):\n",
        "      losses_val = val_loss[epoch][(loop*numbatch_forval_loss)+endloop]*number_val_batchsize\n",
        "      sumy = sumy + losses_val\n",
        "    losses_val = val_loss[epoch][(loop*numbatch_forval_loss)+endloop+1]*number_val_endbatch\n",
        "    sumy = sumy + losses_val\n",
        "    mean = sumy / (((numbatch_forval_loss-1)*number_val_batchsize)+(number_val_endbatch))\n",
        "    val_loss_mean.append(mean)\n",
        "    print('  mean=',mean , '  val_loss_mean.shape',len(val_loss_mean))\n",
        "    sumy=0\n",
        "    mean=0\n",
        "\n",
        "  return train_acc_list,  train_loss_mean,    val_acc_list,  val_loss_mean  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCreVK_5ILlm"
      },
      "source": [
        "## Function To Save The Test Results And Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaNu3IBgIVhV"
      },
      "outputs": [],
      "source": [
        "def final_test_model (model_fn,name,epo_type,data,loss_fn,batch_size):\n",
        "\n",
        "  model=load_model (model_fn ,name,epo_type)\n",
        "\n",
        "  test_loss_h , test_yd_h , test_y_h , test_yp_h =[],[],[],[]\n",
        "  test_loss_e , test_yd_e , test_y_e , test_yp_e = test_func(dataset=data ,model=model , loss=loss_fn ,batch_size=batch_size,epoch=1)\n",
        "\n",
        "  test_loss_h.append(test_loss_e)\n",
        "  test_yd_h.append(test_yd_e)\n",
        "  test_y_h.append(test_y_e)\n",
        "\n",
        " ##########################################################################    test\n",
        "  print('#################################################################    test   ###################################################################') \n",
        "\n",
        "  number_test_loss_epoch=len(test_loss_h[0])\n",
        "  number_test_epoch=len(test_yd_h)\n",
        "  number_test_batch=len(test_yd_h[0])-1\n",
        "  number_test_batchsize=batch_size\n",
        "  number_test_endbatch= len(data) % number_test_batchsize\n",
        "  if (number_test_endbatch==0):\n",
        "    number_test_endbatch=16\n",
        "\n",
        "  test_loss=[]\n",
        "  test_yd=[]\n",
        "  test_y=[]\n",
        "\n",
        "  batch_fix=0\n",
        "  for epoch in range(0,1):\n",
        "    for batch in range(0,number_test_batch):\n",
        "      \n",
        "      batch_fix=batch+1\n",
        "      for size in range(0,number_test_batchsize):\n",
        "        test_yd.append(test_yd_h[epoch][batch][size])\n",
        "        test_y.append(test_y_h[epoch][batch][size])\n",
        "\n",
        "    for size in range(0,number_test_endbatch):\n",
        "      test_yd.append(test_yd_h[epoch][batch_fix][size])\n",
        "      test_y.append(test_y_h[epoch][batch_fix][size])\n",
        "\n",
        "  number_a_epoch=1\n",
        "  number_test_point=1\n",
        "\n",
        "  test_matrix=torch.tensor([[0,0,0],[0,0,0],[0,0,0]])\n",
        "\n",
        "  num_test=0\n",
        "  for id in range(0,len(test_yd)):\n",
        "    test_yd_b=torch.tensor(test_yd[id])\n",
        "    test_y_b=torch.tensor(test_y[id])\n",
        "    # Matrix\n",
        "    if(test_yd_b==0 and test_y_b==0):\n",
        "      test_matrix[0][0]+=1;\n",
        "    elif(test_yd_b==0 and test_y_b==1):\n",
        "      test_matrix[0][1]+=1;\n",
        "    elif(test_yd_b==0 and test_y_b==2):\n",
        "      test_matrix[0][2]+=1;\n",
        "    elif(test_yd_b==1 and test_y_b==0):\n",
        "      test_matrix[1][0]+=1;\n",
        "    elif(test_yd_b==1 and test_y_b==1):\n",
        "      test_matrix[1][1]+=1;\n",
        "    elif(test_yd_b==1 and test_y_b==2):\n",
        "      test_matrix[1][2]+=1;\n",
        "    elif(test_yd_b==2 and test_y_b==0):\n",
        "      test_matrix[2][0]+=1;\n",
        "    elif(test_yd_b==2 and test_y_b==1):\n",
        "      test_matrix[2][1]+=1;\n",
        "    elif(test_yd_b==2 and test_y_b==2):\n",
        "      test_matrix[2][2]+=1;\n",
        "\n",
        "    num_corrects_test=torch.sum(test_yd_b==test_y_b)\n",
        "    num_test+=num_corrects_test\n",
        "    acc_test=num_test/len(test_yd)*100\n",
        "\n",
        "  sumy=0\n",
        "  mean=0\n",
        "\n",
        "  for id in range(0,number_test_batch):\n",
        "    losses_test = test_loss_h[0][id]*batch_size\n",
        "    sumy = sumy + losses_test\n",
        "\n",
        "  if (number_test_endbatch>0):\n",
        "    losses_test = test_loss_h[0][id+1]*number_test_endbatch\n",
        "    sumy = sumy + losses_test\n",
        "\n",
        "  mean = sumy / len(test_yd)\n",
        "  test_loss_all=mean\n",
        "\n",
        "\n",
        "\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  test_yp_e=soft(test_yp_e)\n",
        "  test_y = torch.Tensor(test_y)\n",
        "\n",
        "\n",
        "  test_yp_e = (test_yp_e).to('cpu')\n",
        "  test_yp_e=test_yp_e.float()\n",
        "  test_y = test_y.to('cpu')\n",
        "  test_y=test_y.int()\n",
        "\n",
        "  auroc = MulticlassAUROC(task=\"multiclass\" ,num_classes=3, average=None, thresholds=None)\n",
        "  print('AUC',auroc(test_yp_e, test_y))\n",
        "  auroc = MulticlassAUROC(task=\"multiclass\" ,num_classes=3, average=\"macro\", thresholds=None)\n",
        "  print('AUC',auroc(test_yp_e, test_y))\n",
        "\n",
        "  metric_collection = MetricCollection([\n",
        "    MulticlassAccuracy(num_classes=3, average=\"macro\"),\n",
        "    MulticlassPrecision(num_classes=3, average=\"micro\"),\n",
        "    MulticlassRecall(num_classes=3, average=\"micro\")\n",
        "  ])\n",
        "  print(metric_collection(test_yp_e, test_y))\n",
        "\n",
        "  metric_collection = MetricCollection([\n",
        "    MulticlassAccuracy(num_classes=3, average=None),\n",
        "    MulticlassPrecision(num_classes=3, average=None),\n",
        "    MulticlassRecall(num_classes=3, average=None)\n",
        "  ])\n",
        "  print(metric_collection(test_yp_e, test_y))\n",
        "\n",
        "  print('num_test',num_test)\n",
        "  print('acc_test',acc_test)\n",
        "  print('test_loss_all',test_loss_all)\n",
        "  print('test_matrix')\n",
        "  print(test_matrix)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueIGy-SKjelB"
      },
      "source": [
        "################################################################################\n",
        "# Run CNNs \n",
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The line related to the defined networks should be executed**"
      ],
      "metadata": {
        "id": "zoeXpJ6DdRaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGVFTwGgjelD"
      },
      "outputs": [],
      "source": [
        "run_func('inceptionv3_new' , model_inceptionv3 , loss_fn_ce , optimizer_inceptionv3 , 20 , 16)\n",
        "run_func('res50_result' , model_res50 , loss_fn_ce , optimizer_res50 , 20 , 16)\n",
        "run_func('dense201_result' , model_dense201 , loss_fn_ce , optimizer_dense201 , 20 , 16)\n",
        "run_func('mnas_result' , model_mnas , loss_fn_ce , optimizer_mnas ,20 , 16)\n",
        "run_func('mobilev3_result' , model_mobilev3 , loss_fn_ce , optimizer_mobile , 20 , 16)\n",
        "run_func('efficient_b4_result' , model_effi_b4 , loss_fn_ce , optimizer_effi_b4 , 20 , 16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFAVMDkTHxvq"
      },
      "source": [
        "################################################################################\n",
        "# Plot CNN Results\n",
        "The first step is to Load training and evaluation results. In training, accuracy is computing for each 1024 of images and Loss is computing for each 104 batches, and in Validation, accuracy is computing for each 1344 of images and Loss is computing for each 84 batches.\n",
        "The second step is to Plot the accuracy of the training and evaluation\n",
        "The third step is to plot the Loss of training and evaluation\n",
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrKHf2yMLuJm"
      },
      "source": [
        "## InceptionV3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcRXC8dfLuJx"
      },
      "source": [
        "### Load Training And Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL4tbvRJLuJx"
      },
      "outputs": [],
      "source": [
        "train_acc_inc , train_loss_inc , val_acc_inc , val_loss_inc = acc_mat_func ('inceptionv3_result',1239,104,1344,84,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zX_q1XiLuJy"
      },
      "source": [
        "### Plot Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXJr6cELLuJy"
      },
      "outputs": [],
      "source": [
        "print(len(train_acc_inc))\n",
        "print(len(val_acc_inc))\n",
        "plt.plot(range(1,len(train_acc_inc)+1),train_acc_inc)\n",
        "s_acc=(len(train_acc_inc))/(len(val_acc_inc))\n",
        "a_acc=2.5\n",
        "d_acc=[2.5]\n",
        "for id in range(1,len(val_acc_inc)):\n",
        "  d_acc.append(a_acc+s_acc)\n",
        "  a_acc+=s_acc\n",
        "plt.plot(d_acc,val_acc_inc)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,101)\n",
        "plt.xticks([2.5,6.5,10.5,14.5,18.5,22.5,26.5,30.5,34.5,38.5,42.5,46.5,50.5,54.5,58.5,62.5,66.5,70.5,74.5,78.5],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LekN0CrLuJz"
      },
      "source": [
        "### Plot Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIpu3QDuLuJz"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,len(train_loss_inc)+1),train_loss_inc)\n",
        "print(len(train_loss_inc))\n",
        "print(len(val_loss_inc))\n",
        "s_loss=(len(train_loss_inc))/(len(val_loss_inc))\n",
        "a_loss=2\n",
        "d_loss=[2]\n",
        "for id in range(1,len(val_loss_inc)):\n",
        "  d_loss.append(a_loss+s_loss)\n",
        "  a_loss+=s_loss\n",
        "plt.plot(d_loss,val_loss_inc)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks([2,5,8,11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.ylim(0,5)\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riMjgZwPkFVe"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWfTLjiBkFVf"
      },
      "source": [
        "### Load Training And Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQF8jE8gkFVf"
      },
      "outputs": [],
      "source": [
        "train_acc_res , train_loss_res , val_acc_res , val_loss_res = acc_mat_func ('res50_result',1239,104,1344,84,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzn7tJ5TkFVg"
      },
      "source": [
        "### Plot Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT7EZhuVkFVh"
      },
      "outputs": [],
      "source": [
        "print(len(train_acc_res))\n",
        "print(len(val_acc_res))\n",
        "plt.plot(range(1,len(train_acc_res)+1),train_acc_res)\n",
        "s_acc=(len(train_acc_res))/(len(val_acc_res))\n",
        "a_acc=2.5\n",
        "d_acc=[2.5]\n",
        "for id in range(1,len(val_acc_res)):\n",
        "  d_acc.append(a_acc+s_acc)\n",
        "  a_acc+=s_acc\n",
        "plt.plot(d_acc,val_acc_res)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,101)\n",
        "plt.xticks([2.5,6.5,10.5,14.5,18.5,22.5,26.5,30.5,34.5,38.5,42.5,46.5,50.5,54.5,58.5,62.5,66.5,70.5,74.5,78.5],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh2hXsq7kFVh"
      },
      "source": [
        "### Plot Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq8BHpZRkFVi"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,len(train_loss_res)+1),train_loss_res)\n",
        "print(len(train_loss_res))\n",
        "print(len(val_loss_res))\n",
        "s_loss=(len(train_loss_res))/(len(val_loss_res))\n",
        "a_loss=2\n",
        "d_loss=[2]\n",
        "for id in range(1,len(val_loss_res)):\n",
        "  d_loss.append(a_loss+s_loss)\n",
        "  a_loss+=s_loss\n",
        "plt.plot(d_loss,val_loss_res)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks([2,5,8,11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.ylim(0,5)\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ech4SnqwkdfJ"
      },
      "source": [
        "## DenseNet201"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqi4NPX1kdfK"
      },
      "source": [
        "### Load Training And Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM4RvEGOkdfL"
      },
      "outputs": [],
      "source": [
        "train_acc_dense , train_loss_dense , val_acc_dense , val_loss_dense = acc_mat_func ('dense201_result',1239,104,1344,84,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ot3-cxGkdfM"
      },
      "source": [
        "### Plot Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7MAIPrukdfM"
      },
      "outputs": [],
      "source": [
        "print(len(train_acc_dense))\n",
        "print(len(val_acc_dense))\n",
        "plt.plot(range(1,len(train_acc_dense)+1),train_acc_dense)\n",
        "s_acc=(len(train_acc_dense))/(len(val_acc_dense))\n",
        "a_acc=2.5\n",
        "d_acc=[2.5]\n",
        "for id in range(1,len(val_acc_dense)):\n",
        "  d_acc.append(a_acc+s_acc)\n",
        "  a_acc+=s_acc\n",
        "plt.plot(d_acc,val_acc_dense)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,101)\n",
        "plt.xticks([2.5,6.5,10.5,14.5,18.5,22.5,26.5,30.5,34.5,38.5,42.5,46.5,50.5,54.5,58.5,62.5,66.5,70.5,74.5,78.5],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48q1uvevkdfN"
      },
      "source": [
        "### Plot Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aql30LyYkdfO"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,len(train_loss_dense)+1),train_loss_dense)\n",
        "print(len(train_loss_dense))\n",
        "print(len(val_loss_dense))\n",
        "s_loss=(len(train_loss_dense))/(len(val_loss_dense))\n",
        "a_loss=2\n",
        "d_loss=[2]\n",
        "for id in range(1,len(val_loss_dense)):\n",
        "  d_loss.append(a_loss+s_loss)\n",
        "  a_loss+=s_loss\n",
        "plt.plot(d_loss,val_loss_dense)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks([2,5,8,11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.ylim(0,5)\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-hoFWzHkujA"
      },
      "source": [
        "## MnasNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkUdklGRkujD"
      },
      "source": [
        "### Load Training And Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdbY9V-bkujE"
      },
      "outputs": [],
      "source": [
        "train_acc_mnas , train_loss_mnas , val_acc_mnas , val_loss_mnas = acc_mat_func ('mnas_result',1239,104,1344,84,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQyLXaR0kujF"
      },
      "source": [
        "### Plot Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkT045ctkujG"
      },
      "outputs": [],
      "source": [
        "print(len(train_acc_mnas))\n",
        "print(len(val_acc_mnas))\n",
        "plt.plot(range(1,len(train_acc_mnas)+1),train_acc_mnas)\n",
        "s_acc=(len(train_acc_mnas))/(len(val_acc_mnas))\n",
        "a_acc=2.5\n",
        "d_acc=[2.5]\n",
        "for id in range(1,len(val_acc_mnas)):\n",
        "  d_acc.append(a_acc+s_acc)\n",
        "  a_acc+=s_acc\n",
        "plt.plot(d_acc,val_acc_mnas)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,101)\n",
        "plt.xticks([2.5,6.5,10.5,14.5,18.5,22.5,26.5,30.5,34.5,38.5,42.5,46.5,50.5,54.5,58.5,62.5,66.5,70.5,74.5,78.5],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbl95JHCkujI"
      },
      "source": [
        "### Plot Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X370y09RkujJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,len(train_loss_mnas)+1),train_loss_mnas)\n",
        "print(len(train_loss_mnas))\n",
        "print(len(val_loss_mnas))\n",
        "s_loss=(len(train_loss_mnas))/(len(val_loss_mnas))\n",
        "a_loss=2\n",
        "d_loss=[2]\n",
        "for id in range(1,len(val_loss_mnas)):\n",
        "  d_loss.append(a_loss+s_loss)\n",
        "  a_loss+=s_loss\n",
        "plt.plot(d_loss,val_loss_mnas)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks([2,5,8,11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.ylim(0,5)\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1KBYk0ik_mP"
      },
      "source": [
        "## MobileNetV3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0wsp2e2k_mQ"
      },
      "source": [
        "### Load Training And Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqlmHZ8vk_mR"
      },
      "outputs": [],
      "source": [
        "train_acc_mob , train_loss_mob , val_acc_mob , val_loss_mob = acc_mat_func ('mobilev3_result',1239,104,1344,84,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As_39jd_k_mS"
      },
      "source": [
        "### Plot Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpXwv6GXk_mS"
      },
      "outputs": [],
      "source": [
        "print(len(train_acc_mob))\n",
        "print(len(val_acc_mob))\n",
        "plt.plot(range(1,len(train_acc_mob)+1),train_acc_mob)\n",
        "s_acc=(len(train_acc_mob))/(len(val_acc_mob))\n",
        "a_acc=2.5\n",
        "d_acc=[2.5]\n",
        "for id in range(1,len(val_acc_mob)):\n",
        "  d_acc.append(a_acc+s_acc)\n",
        "  a_acc+=s_acc\n",
        "plt.plot(d_acc,val_acc_mob)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,101)\n",
        "plt.xticks([2.5,6.5,10.5,14.5,18.5,22.5,26.5,30.5,34.5,38.5,42.5,46.5,50.5,54.5,58.5,62.5,66.5,70.5,74.5,78.5],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzicBHXEk_mT"
      },
      "source": [
        "### Plot Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqif_cf0k_mU"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,len(train_loss_mob)+1),train_loss_mob)\n",
        "print(len(train_loss_mob))\n",
        "print(len(val_loss_mob))\n",
        "s_loss=(len(train_loss_mob))/(len(val_loss_mob))\n",
        "a_loss=2\n",
        "d_loss=[2]\n",
        "for id in range(1,len(val_loss_mob)):\n",
        "  d_loss.append(a_loss+s_loss)\n",
        "  a_loss+=s_loss\n",
        "plt.plot(d_loss,val_loss_mob)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks([2,5,8,11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.ylim(0,5)\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlQx5WW7lRIM"
      },
      "source": [
        "## EfficientNet_B4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKK5PsYPlRIN"
      },
      "source": [
        "### Load Training And Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr1tkiCylRIO"
      },
      "outputs": [],
      "source": [
        "train_acc_effi , train_loss_effi , val_acc_effi , val_loss_effi = acc_mat_func ('efficient_b4_result',1239,104,1344,84)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shDs7ADmlRIO"
      },
      "source": [
        "### Plot Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kik2lzeOlRIP"
      },
      "outputs": [],
      "source": [
        "print(len(train_acc_effi))\n",
        "print(len(val_acc_effi))\n",
        "plt.plot(range(1,len(train_acc_effi)+1),train_acc_effi)\n",
        "s_acc=(len(train_acc_effi))/(len(val_acc_effi))\n",
        "a_acc=2.5\n",
        "d_acc=[2.5]\n",
        "for id in range(1,len(val_acc_effi)):\n",
        "  d_acc.append(a_acc+s_acc)\n",
        "  a_acc+=s_acc\n",
        "plt.plot(d_acc,val_acc_effi)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,101)\n",
        "plt.xticks([2.5,6.5,10.5,14.5,18.5,22.5,26.5,30.5,34.5,38.5,42.5,46.5,50.5,54.5,58.5,62.5,66.5,70.5,74.5,78.5],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC9Z4sWolRIQ"
      },
      "source": [
        "### Plot Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZx8dbbwlRIQ"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,len(train_loss_effi)+1),train_loss_effi)\n",
        "s_loss=(len(train_loss_effi))/(len(val_loss_effi))\n",
        "a_loss=2\n",
        "d_loss=[2]\n",
        "for id in range(1,len(val_loss_effi)):\n",
        "  d_loss.append(a_loss+s_loss)\n",
        "  a_loss+=s_loss\n",
        "plt.plot(d_loss,val_loss_effi)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks([2,5,8,11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.ylim(0,5)\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-uTgdF7MlSA"
      },
      "source": [
        "## All Networks in A Chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozO9vemMlSD"
      },
      "source": [
        "### Plot Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hie-vPMLMlSE"
      },
      "outputs": [],
      "source": [
        "s_acc=(len(train_acc_effi))/(len(val_acc_effi))\n",
        "a_acc=2.5\n",
        "d_acc=[2.5]\n",
        "for id in range(1,len(val_acc_effi)):\n",
        "  d_acc.append(a_acc+s_acc)\n",
        "  a_acc+=s_acc\n",
        "plt.plot(d_acc,val_acc_inc)\n",
        "plt.plot(d_acc,val_acc_res)\n",
        "plt.plot(d_acc,val_acc_dense)\n",
        "plt.plot(d_acc,val_acc_mnas)\n",
        "plt.plot(d_acc,val_acc_mob)\n",
        "plt.plot(d_acc,val_acc_effi)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(80,101)\n",
        "plt.xticks([2.5,6.5,10.5,14.5,18.5,22.5,26.5,30.5,34.5,38.5,42.5,46.5,50.5,54.5,58.5,62.5,66.5,70.5,74.5,78.5],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.legend(['InceptionV3','ResNet50','DenseNet201','MnasNet','MobileNetV3','EfficientNet_B4'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwTn5mFPMlSF"
      },
      "source": [
        "### Plot Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg-kjOevMlSG"
      },
      "outputs": [],
      "source": [
        "s_loss=(len(train_loss_effi))/(len(val_loss_effi))\n",
        "a_loss=2\n",
        "d_loss=[2]\n",
        "for id in range(1,len(val_loss_effi)):\n",
        "  d_loss.append(a_loss+s_loss)\n",
        "  a_loss+=s_loss\n",
        "plt.plot(d_loss,val_loss_inc)\n",
        "plt.plot(d_loss,val_loss_res)\n",
        "plt.plot(d_loss,val_loss_dense)\n",
        "plt.plot(d_loss,val_loss_mnas)\n",
        "plt.plot(d_loss,val_loss_mob)\n",
        "plt.plot(d_loss,val_loss_effi)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks([2,5,8,11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.ylim(0,1)\n",
        "plt.legend(['InceptionV3','ResNet50','DenseNet201','MnasNet','MobileNetV3','EfficientNet_B4'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epNUyLUjHhRA"
      },
      "source": [
        "################################################################################\n",
        "# Test Models And Plot Confusion Matrix\n",
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The line related to the defined networks should be executed**"
      ],
      "metadata": {
        "id": "f9NcTOc_fYMz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFYZmfB6N53o"
      },
      "outputs": [],
      "source": [
        "final_test_model (model_inceptionv3,'inception_RGB_256',12,test_Book,loss_fn_ce,16)\n",
        "final_test_model (model_res50,'res50_256',20,test_Book,loss_fn_ce,16)\n",
        "final_test_model (model_dense201,'dense201_256',18,test_Book,loss_fn_ce,16)\n",
        "final_test_model (model_mnas,'mnas_0.1_RGB_256',20,test_Book,loss_fn_ce,16)\n",
        "final_test_model (model_mobilev3,'mobilev3_large_256',17,test_Book,loss_fn_ce,16)\n",
        "final_test_model (model_effi_b4,'efficient_b4_256',19,test_Book,loss_fn_ce,16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGO8vNN4ljHv"
      },
      "source": [
        "################################################################################\n",
        "# Define Functions For Fully Connected Layer First And Run And Save Results\n",
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsUvvKJVljHx"
      },
      "source": [
        "## Function To Training Fully Connected Layer First"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPb9UscXljHz"
      },
      "outputs": [],
      "source": [
        "def train_f(dataset, model, loss ,optimizer,batch_size,epoch):\n",
        "\n",
        "  vol=len(dataset)\n",
        "  iter_s = vol // batch_size\n",
        "  iter_f = vol % batch_size\n",
        "  s=[]\n",
        "  t=[]\n",
        "  l=[]\n",
        "  lossy=[]\n",
        "\n",
        "  for idx in range(0,iter_s):\n",
        "    number_batch=int(idx)+1\n",
        "    print('epoch=' ,epoch ,'batch = ',number_batch)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=batch_size)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=batch_size)\n",
        "\n",
        "    for (X,y) in data_loader:\n",
        "        model.train\n",
        "        X, y = X.to(dev), y.to(dev)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred,x = model(X)\n",
        "        loss_value = loss(pred, x)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        lossy.append(loss_value.item())\n",
        "        \n",
        "        print('   Train Loss:', loss_value.item())    \n",
        "    ########################################################################################################\n",
        "  if iter_f > 0 :\n",
        "    print('epoch=' ,epoch ,'last batch = ' ,number_batch+1)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=iter_f)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=iter_f)\n",
        "\n",
        "    for (X,y) in data_loader:\n",
        "        model.train\n",
        "        X, y = X.to(dev), y.to(dev)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred,x = model(X)\n",
        "        loss_value = loss(pred, x)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        lossy.append(loss_value.item())\n",
        "        s.append(x.cpu().detach().numpy())\n",
        "        t.append(pred.cpu().detach().numpy())\n",
        "        l.append(y.cpu().detach().numpy())\n",
        "        \n",
        "        print('   Train Loss:', loss_value.item())  \n",
        "\n",
        "  return s , t ,l, lossy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL5J6hLdJDWF"
      },
      "source": [
        "## Function To Validation and Testing Fully Connected Layer First"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSfGYRvQJDWG"
      },
      "outputs": [],
      "source": [
        "def test_f(dataset, model, loss,batch_size,epoch):\n",
        "\n",
        "  model.eval()\n",
        "  vol=len(dataset)\n",
        "  iter_s = vol // batch_size\n",
        "  iter_f = vol % batch_size\n",
        "  s=[]\n",
        "  t=[]\n",
        "  l=[]\n",
        "  loss_all=[]\n",
        "\n",
        "  for idx in range(0,iter_s):\n",
        "    number_batch=int(idx)+1\n",
        "    print('epoch=' ,epoch ,'batch = ',number_batch)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=batch_size)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for (X,y) in data_loader:\n",
        "        # Compute prediction error\n",
        "        pred,x = model(X)\n",
        "        loss_value = loss(pred,x)\n",
        "        loss_all.append(loss_value.detach().cpu().numpy().item())\n",
        "        print('   val Loss:', loss_value.item())\n",
        "\n",
        "    ########################################################################################################\n",
        "  if iter_f > 0 :\n",
        "    print('epoch=' ,epoch ,'last batch = ' ,number_batch+1)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=iter_f)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=iter_f)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for (X,y) in data_loader:\n",
        "        # Compute prediction error\n",
        "\n",
        "        pred,x = model(X)\n",
        "        loss_value = loss(pred,x)\n",
        "        loss_all.append(loss_value.detach().cpu().numpy().item())\n",
        "        s.append(x.cpu().detach().numpy())\n",
        "        t.append(pred.cpu().detach().numpy())\n",
        "        l.append(y.cpu().detach().numpy())\n",
        "        print('   val Loss:', loss_value.item())\n",
        "\n",
        "  return s , t ,l, loss_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46HxE5lDQ48U"
      },
      "source": [
        "## Run Fully Connected Layer First And Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "763n8ECcQ8MA"
      },
      "outputs": [],
      "source": [
        "tr_f_loss=[]\n",
        "val_f_loss=[]\n",
        "for epo in range(0,20):\n",
        "    \n",
        "    print(f\"Epoch {epo+1}\\n-------------------------------\")\n",
        "    x_tr, y_tr, l_tr, loss_tr = train_f(train_Book, model_f, loss_fn_mse, optimizer_f,16,epo+1)\n",
        "    tr_f_loss.append(loss_tr)\n",
        "\n",
        "    print('valllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll')\n",
        "    x_val, y_val, l_val, loss_val = test_f(val_Book, model_f, loss_fn_mse ,16,epo+1)\n",
        "    val_f_loss.append(loss_val)\n",
        "\n",
        "torch.save(model_f.state_dict(), '/content/drive/MyDrive/modelsaved/saved_model_model_f'+str(epo+1)+'.pth')\n",
        "\n",
        "np.save('/content/drive/MyDrive/model_f_loss_train'   , tr_f_loss)\n",
        "np.save('/content/drive/MyDrive/model_f_loss_val'   , val_f_loss)\n",
        "\n",
        "np.save('/content/drive/MyDrive/model_f_x_train'   , x_tr)\n",
        "np.save('/content/drive/MyDrive/model_f_y_train'   , y_tr)\n",
        "np.save('/content/drive/MyDrive/model_f_l_train'   , l_tr)\n",
        "\n",
        "np.save('/content/drive/MyDrive/model_f_x_val'   , x_val)\n",
        "np.save('/content/drive/MyDrive/model_f_y_val'   , y_val)\n",
        "np.save('/content/drive/MyDrive/model_f_l_val'   , l_val)\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1griK4gNvZNF"
      },
      "source": [
        "## Plot Result of Fully Connected Layer First"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD2WBhz4QNgf"
      },
      "outputs": [],
      "source": [
        "tr_ls=np.load('/content/drive/MyDrive/model_f_loss_train.npy',allow_pickle=True)\n",
        "val_ls=np.load('/content/drive/MyDrive/model_f_loss_val.npy',allow_pickle=True)\n",
        "\n",
        "t=tr_ls\n",
        "v=val_ls\n",
        "\n",
        "tiri=[]\n",
        "vali=[]\n",
        "for i in range(0,20):\n",
        "  for j in range(0,310):\n",
        "    tiri.append(t[i][j])\n",
        "\n",
        "for i in range(0,20):\n",
        "  for j in range(0,84):\n",
        "    vali.append(v[i][j])\n",
        "\n",
        "trim=[]\n",
        "valim=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "for i in range(0,80):\n",
        "  gt=vali[78*i:78*(i+1)]\n",
        "  mt=np.mean(gt)\n",
        "  trim.append(mt)\n",
        "\n",
        "for i in range(0,20):\n",
        "  gv=vali[84*i:84*(i+1)]\n",
        "  mv=np.mean(gv)\n",
        "  valim[i]=mv\n",
        "\n",
        "plt.plot(range(1,len(tiri)+1),tiri)\n",
        "s_loss=(len(tiri))/(len(valim))\n",
        "a_loss=2\n",
        "d_loss=[2]\n",
        "for id in range(1,len(valim)):\n",
        "  d_loss.append(a_loss+s_loss)\n",
        "  a_loss+=s_loss\n",
        "plt.plot(d_loss,valim)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks([0,310,620,930,1240,1550,1860,2170,2480,2790,3100,3410,3720,4030,4340,4650,4960,5270,5580,5890],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.ylim(0,0.05)\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd-_ObZEvEPM"
      },
      "source": [
        "################################################################################\n",
        "# Define Functions For FCLF-CNN And Run And Save Results\n",
        "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGe8mvCBljIA"
      },
      "source": [
        "## Function To Training FCLF-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elxIO4ulljIB"
      },
      "outputs": [],
      "source": [
        "def train_fclf(dataset, model1,model2, loss_fn1,loss_fn2, optimizer1,optimizer2,batch_size,epoch):\n",
        "\n",
        "  model1.eval()\n",
        "\n",
        "  vol=len(dataset)\n",
        "  iter_s = vol // batch_size\n",
        "  iter_f = vol % batch_size\n",
        "  \n",
        "  number_batch=0\n",
        "  \n",
        "  loss_all_lst=[]\n",
        "  yd_all=[]\n",
        "  y_all=[]\n",
        "\n",
        "  for idx in range(0,iter_s):\n",
        "    number_batch=int(idx)+1\n",
        "    print('epoch=' ,epoch ,'batch = ',number_batch)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=batch_size)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=batch_size)\n",
        "\n",
        "    for (x,y) in data_loader:\n",
        "      model2.train()\n",
        "\n",
        "      # Compute prediction error f\n",
        "      pred,x = model1(x)\n",
        "      loss1 = loss_fn1(pred, x)\n",
        "      pred=pred.reshape(len(x),3,256,256)\n",
        "\n",
        "      # Compute prediction error cnn\n",
        "      yp = model2(pred.clone())\n",
        "      loss2 = loss_fn2(yp, y)\n",
        "      loss_all=loss1+loss2\n",
        "\n",
        "      optimizer2.zero_grad()\n",
        "      loss2.backward(inputs=list(model2.parameters()))\n",
        "      optimizer2.step()\n",
        "\n",
        "      # get accuracy\n",
        "      yd=(torch.max(yp,1)[1])\n",
        "      print('(',yd,')','|' , '(',y,')')\n",
        "\n",
        "      num_corrects=torch.sum(torch.max(yp,1)[1]==y)\n",
        "      acc = num_corrects.float()/float(batch_size)\n",
        "\n",
        "      loss_all_lst.append(loss_all.detach().cpu().numpy().item())\n",
        "      yd_all.append(yd.cpu().numpy())\n",
        "      y_all.append(y.cpu().numpy())\n",
        "  \n",
        "      print('num_corrects:' , num_corrects.item(),', Train Loss_f: ', loss1.item(), ', Train Loss_c: ', loss2.item(),\n",
        "            ', Train Loss all:', loss_all.item() , ', Train Accuracy:' , acc.item())\n",
        "########################################################################################################\n",
        "  if iter_f > 0 :\n",
        "    print('epoch=' ,epoch ,'last batch = ' ,number_batch+1)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=iter_f)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=iter_f)\n",
        "\n",
        "    for (x,y) in data_loader:\n",
        "      model2.train()\n",
        "\n",
        "      # Compute prediction error f\n",
        "      pred,x = model1(x)\n",
        "      loss1 = loss_fn1(pred, x)\n",
        "      pred=pred.reshape(len(x),3,256,256)\n",
        "\n",
        "      # Compute prediction error cnn\n",
        "      yp = model2(pred.clone())\n",
        "      loss2 = loss_fn2(yp, y)\n",
        "\n",
        "      loss_all=loss1+loss2\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer2.zero_grad()\n",
        "      loss2.backward(inputs=list(model2.parameters()))\n",
        "      optimizer2.step()\n",
        "\n",
        "      # get accuracy\n",
        "      yd=(torch.max(yp,1)[1])\n",
        "      print('(',yd,')','|' , '(',y,')')\n",
        "\n",
        "      num_corrects=torch.sum(torch.max(yp,1)[1]==y)\n",
        "      acc = num_corrects.float()/float(batch_size)\n",
        "\n",
        "      loss_all_lst.append(loss_all.detach().cpu().numpy().item())\n",
        "      yd_all.append(yd.cpu().numpy())\n",
        "      y_all.append(y.cpu().numpy())\n",
        "  \n",
        "      print('num_corrects:' , num_corrects.item(),', Train Loss 1:', loss1.item(), ', Train Loss 2:', loss2.item(),\n",
        "            ', Train Loss all:', loss_all.item() , ', Train Accuracy:' , acc.item())\n",
        "      \n",
        "  return  loss_all_lst , yd_all , y_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHrCVCr80yy-"
      },
      "source": [
        "## Function To Validation and Testing FCLF-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2r7SXZa0yzL"
      },
      "outputs": [],
      "source": [
        "def test_fclf(dataset , model1 , model2 , loss_fn1 , loss_fn2 , batch_size , epoch):\n",
        "  model1.eval()\n",
        "  model2.eval()\n",
        "\n",
        "  vol=len(dataset)\n",
        "  iter_s = vol // batch_size\n",
        "  iter_f = vol % batch_size\n",
        "  \n",
        "  number_batch=0\n",
        "\n",
        "  loss1_lst=[]\n",
        "  loss2_lst=[]\n",
        "  loss_all_lst=[]\n",
        "  yd_all=[]\n",
        "  y_all=[]\n",
        "\n",
        "  loss_value=0\n",
        "  for idx in range(0,iter_s):\n",
        "    number_batch=int(idx)+1\n",
        "    print('epoch=' ,epoch ,'batch = ',number_batch)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=batch_size)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=batch_size)\n",
        " \n",
        "    with torch.no_grad():\n",
        "     for (x,y) in data_loader:\n",
        "\n",
        "      # Compute prediction error f\n",
        "      pred,x = model1(x)\n",
        "      loss1 = loss_fn1(pred, x)\n",
        "      pred=pred.reshape(len(x),3,256,256)\n",
        "\n",
        "      # Compute prediction error cnn\n",
        "      yp = model2(pred)\n",
        "      loss2 = loss_fn2(yp, y)\n",
        "\n",
        "      loss_all=loss1+loss2\n",
        "\n",
        "      # get accuracy\n",
        "      yd=(torch.max(yp,1)[1])\n",
        "      print('(',yd,')','|' , '(',y,')')\n",
        "\n",
        "      num_corrects=torch.sum(torch.max(yp,1)[1]==y)\n",
        "      acc = num_corrects.float()/float(batch_size)\n",
        "\n",
        "      # make lists\n",
        "      loss1_lst.append(loss1.detach().cpu().numpy().item())\n",
        "      loss2_lst.append(loss2.detach().cpu().numpy().item())\n",
        "      loss_all_lst.append(loss_all.detach().cpu().numpy().item())\n",
        "      yd_all.append(yd.cpu().numpy())\n",
        "      y_all.append(y.cpu().numpy())\n",
        "  \n",
        "      print('num_corrects:' , num_corrects.item(),', val Loss 1:', loss1.item(), ', val Loss 2:', loss2.item(),\n",
        "            ', val Loss all:', loss_all.item() , ', val Accuracy:' , acc.item())\n",
        "########################################################################################################\n",
        "  if iter_f > 0 :\n",
        "    print('epoch=' ,epoch ,'last batch = ' ,number_batch+1)\n",
        "    batch_images=custom_data(root='/content',book=dataset,size=iter_f)\n",
        "    images,lables=batch_images._get()\n",
        "    dataset=batch_images._remove()\n",
        "    data_loader=data.DataLoader(dataset=images,batch_size=iter_f)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "     for (x,y) in data_loader:\n",
        "\n",
        "      # Compute prediction error f\n",
        "      pred,x = model1(x)\n",
        "      loss1 = loss_fn1(pred, x)\n",
        "      pred=pred.reshape(len(x),3,256,256)\n",
        "\n",
        "      # Compute prediction error cnn\n",
        "      yp = model2(pred)\n",
        "      loss2 = loss_fn2(yp, y)\n",
        "\n",
        "      loss_all=loss1+loss2\n",
        "\n",
        "      # get accuracy\n",
        "      yd=(torch.max(yp,1)[1])\n",
        "      print('(',yd,')','|' , '(',y,')')\n",
        "\n",
        "      num_corrects=torch.sum(torch.max(yp,1)[1]==y)\n",
        "      acc = num_corrects.float()/float(batch_size)\n",
        "\n",
        "      # make lists\n",
        "      loss1_lst.append(loss1.detach().cpu().numpy().item())\n",
        "      loss2_lst.append(loss2.detach().cpu().numpy().item())\n",
        "      loss_all_lst.append(loss_all.detach().cpu().numpy().item())\n",
        "      yd_all.append(yd.cpu().numpy())\n",
        "      y_all.append(y.cpu().numpy())\n",
        "  \n",
        "      print('num_corrects:' , num_corrects.item(),', val Loss 1:', loss1.item(), ', val Loss 2:', loss2.item(),\n",
        "            ', val Loss all:', loss_all.item() , ', val Accuracy:' , acc.item())\n",
        "\n",
        "  return  loss2_lst , yd_all , y_all"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load CNN And Fully Connected Layer First parameters"
      ],
      "metadata": {
        "id": "pvufj8qCvfrI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcBhhmBEGAb9"
      },
      "outputs": [],
      "source": [
        "model_f.load_state_dict(torch.load('/content/drive/MyDrive/modelsaved/saved_model_model_f20.pth'))\n",
        "model_effi_b4=load_model (model_effi_b4 ,'efficient_b4_result',19)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZeyA5qJ56XZ"
      },
      "source": [
        "## Run FCLF-CNN And Save Models And Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2Nmud9q56Xa"
      },
      "outputs": [],
      "source": [
        "name='3*1024_fTrained_cnnTrained_fFix'\n",
        "epoch=1\n",
        "\n",
        "tr_loss_all , tr_yd , tr_y =[],[],[]\n",
        "val_loss_all , val_yd , val_y =[],[],[]\n",
        "\n",
        "# train and val\n",
        "for epo in range(0,epoch):\n",
        "\n",
        "    print(' ')\n",
        "    print('epoch ========================================================================================= ',epo+1)\n",
        "    print(' ')\n",
        "    print('trainnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn')\n",
        "    print(' ')\n",
        "\n",
        "    tr_loss_e_all , tr_yd_e , tr_y_e = train_fclf(train_Book, model_f , model_effi_b4 , loss_fn_mse , loss_fn_ce , optimizer_f , optimizer_effi_b4 ,16, epo+1)\n",
        "    print(' ')\n",
        "    print('train=',torch.cuda.memory_allocated(device=dev))\n",
        "\n",
        "    torch.save(model_effi_b4.state_dict(), '/content/drive/MyDrive/modelsaved/saved_model_fclf'+name+'_epoch='+str(epo+1)+'.pth')\n",
        "\n",
        "    print('valllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll')\n",
        "    print(' ')\n",
        "\n",
        "    val_loss_e_all , val_yd_e , val_y_e = test_fclf(val_Book, model_f , model_effi_b4 , loss_fn_mse , loss_fn_ce ,16, epo+1)\n",
        "\n",
        "    tr_loss_all.append(tr_loss_e_all)\n",
        "    tr_yd.append(tr_yd_e)\n",
        "    tr_y.append(tr_y_e)\n",
        "\n",
        "    val_loss_all.append(val_loss_e_all)\n",
        "    val_yd.append(val_yd_e)\n",
        "    val_y.append(val_y_e)\n",
        "\n",
        "np.savez('/content/drive/MyDrive/train_result_fclf_'+name   , tr_loss_all , tr_yd , tr_y)\n",
        "np.savez('/content/drive/MyDrive/val_result_fclf_'+name     , val_loss_all , val_yd , val_y)\n",
        "  \n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uarDIbgMpgkz"
      },
      "source": [
        "## Plot FCLF-CNN Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebMmyGhcpgk1"
      },
      "source": [
        "### Load Training And Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPm9qQVapgk1"
      },
      "outputs": [],
      "source": [
        "train_acc_fclf , train_loss_fclf , val_acc_fclf , val_loss_fclf = acc_mat_func ('fclf_3*1024_fTrained_cnnTrained_fFix',1239,78,1344,84)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATIzS1LRpgk4"
      },
      "source": [
        "### Plot Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_acc_fclf))\n",
        "print(len(val_acc_fclf))\n",
        "plt.plot(range(1,len(train_acc_fclf)+1),train_acc_fclf)\n",
        "s_acc=(len(train_acc_fclf))/(len(val_acc_fclf))\n",
        "a_acc=2.5\n",
        "d_acc=[2.5]\n",
        "for id in range(1,len(val_acc_fclf)):\n",
        "  d_acc.append(a_acc+s_acc)\n",
        "  a_acc+=s_acc\n",
        "plt.plot(d_acc,val_acc_fclf)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0,101)\n",
        "plt.xticks([2.5,6.5,10.5,14.5,18.5,22.5,26.5,30.5,34.5,38.5,42.5,46.5,50.5,54.5,58.5,62.5,66.5,70.5,74.5,78.5],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.legend(['train','val'])"
      ],
      "metadata": {
        "id": "FAOfX2NDJI2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziEx9lIUpgk6"
      },
      "source": [
        "### Plot Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oilMSj3ypgk_"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,len(train_loss_effi)+1),train_loss_effi)\n",
        "print(len(train_loss_effi))\n",
        "print(len(val_loss_effi))\n",
        "s_loss=(len(train_loss_effi))/(len(val_loss_effi))\n",
        "a_loss=2\n",
        "d_loss=[2]\n",
        "for id in range(1,len(val_loss_effi)):\n",
        "  d_loss.append(a_loss+s_loss)\n",
        "  a_loss+=s_loss\n",
        "plt.plot(d_loss,val_loss_effi)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks([2,5,8,11,14,17,20,23,26,29,32,35,38,41,44,47,50,53,56,59],\n",
        "           ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])\n",
        "plt.ylim(0,5)\n",
        "plt.legend(['train','val'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6U1s7RaMdzk"
      },
      "source": [
        "## Test FCLF-CNN And Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pcJklxSMdzl"
      },
      "outputs": [],
      "source": [
        "model_f.load_state_dict(torch.load('/content/drive/MyDrive/modelsaved/saved_model_model_f1.pth'))\n",
        "model_effi_b4.load_state_dict(torch.load('/content/drive/MyDrive/modelsaved/saved_model_fclf3*1024_fTrained_cnnTrained_fFix_epoch=1.pth'))\n",
        "\n",
        "test_loss_h , test_yd_h , test_y_h =[],[],[]\n",
        "test_loss_e , test_yd_e , test_y_e = test_fclf(test_Book, model_f , model_effi_b4 , loss_fn_mse , loss_fn_ce ,16, 1)\n",
        "\n",
        "test_loss_h.append(test_loss_e)\n",
        "test_yd_h.append(test_yd_e)\n",
        "test_y_h.append(test_y_e)\n",
        "\n",
        " ##########################################################################    test\n",
        "print('#################################################################    test   ###################################################################') \n",
        "\n",
        "number_test_loss_epoch=len(test_loss_h[0])\n",
        "number_test_epoch=len(test_yd_h)\n",
        "number_test_batch=len(test_yd_h[0])-1\n",
        "number_test_batchsize=16\n",
        "number_test_endbatch= 2100 % number_test_batchsize\n",
        "if (number_test_endbatch==0):\n",
        "  number_test_endbatch=16\n",
        "\n",
        "test_loss=[]\n",
        "test_yd=[]\n",
        "test_y=[]\n",
        "\n",
        "batch_fix=0\n",
        "for epoch in range(0,1):\n",
        "    for batch in range(0,number_test_batch):\n",
        "      \n",
        "      batch_fix=batch+1\n",
        "      for size in range(0,number_test_batchsize):\n",
        "        test_yd.append(test_yd_h[epoch][batch][size])\n",
        "        test_y.append(test_y_h[epoch][batch][size])\n",
        "\n",
        "    for size in range(0,number_test_endbatch):\n",
        "      test_yd.append(test_yd_h[epoch][batch_fix][size])\n",
        "      test_y.append(test_y_h[epoch][batch_fix][size])\n",
        "\n",
        "number_a_epoch=1\n",
        "number_test_point=1\n",
        "\n",
        "test_matrix=torch.tensor([[0,0,0],[0,0,0],[0,0,0]])\n",
        "\n",
        "num_test=0\n",
        "for id in range(0,len(test_yd)):\n",
        "    test_yd_b=torch.tensor(test_yd[id])\n",
        "    test_y_b=torch.tensor(test_y[id])\n",
        "    # Matrix\n",
        "    if(test_yd_b==0 and test_y_b==0):\n",
        "      test_matrix[0][0]+=1;\n",
        "    elif(test_yd_b==0 and test_y_b==1):\n",
        "      test_matrix[0][1]+=1;\n",
        "    elif(test_yd_b==0 and test_y_b==2):\n",
        "      test_matrix[0][2]+=1;\n",
        "    elif(test_yd_b==1 and test_y_b==0):\n",
        "      test_matrix[1][0]+=1;\n",
        "    elif(test_yd_b==1 and test_y_b==1):\n",
        "      test_matrix[1][1]+=1;\n",
        "    elif(test_yd_b==1 and test_y_b==2):\n",
        "      test_matrix[1][2]+=1;\n",
        "    elif(test_yd_b==2 and test_y_b==0):\n",
        "      test_matrix[2][0]+=1;\n",
        "    elif(test_yd_b==2 and test_y_b==1):\n",
        "      test_matrix[2][1]+=1;\n",
        "    elif(test_yd_b==2 and test_y_b==2):\n",
        "      test_matrix[2][2]+=1;\n",
        "\n",
        "    num_corrects_test=torch.sum(test_yd_b==test_y_b)\n",
        "    num_test+=num_corrects_test\n",
        "\n",
        "    acc_test=num_test/len(test_yd)*100\n",
        "\n",
        "sumy=0\n",
        "mean=0\n",
        "for id in range(0,number_test_batch):\n",
        "\n",
        "    losses_test = test_loss_h[0][id]*16\n",
        "    sumy = sumy + losses_test\n",
        "\n",
        "if (number_test_endbatch>0):\n",
        "    losses_test = test_loss_h[0][id+1]*number_test_endbatch\n",
        "    sumy = sumy + losses_test\n",
        "\n",
        "mean = sumy / len(test_yd)\n",
        "test_loss_all=mean\n",
        "\n",
        "print('num_test',num_test)\n",
        "print('acc_test',acc_test)\n",
        "print('test_loss_all',test_loss_all)\n",
        "print('test_matrix')\n",
        "print(test_matrix)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SylRsyAKfcXb",
        "snD9TpWm3jMn",
        "NkuEg3703qJd",
        "2ww8ALGAmGjz",
        "HuJzqtrVE0AC",
        "li9IJ8KPaC0f",
        "j78gTtzRzg53",
        "rCkLEWsGmsyV",
        "-_GNx-beBhKm",
        "aYCT4vAdBhKs",
        "qGJG5qaIBhK7",
        "-jEvLgjsBhLB",
        "Ylbg6eKiBhKh",
        "5OUgvN2ZBhLG",
        "Tyr028SbBhLH",
        "h8sdU22qKYjf",
        "7vHGi13btaGR",
        "s4hRm6Lxaf8I",
        "-lWaYuWLgm9c",
        "YweHR3oW6zfx",
        "5S-X1QENAC6n",
        "4zaktL-AM68C",
        "G54hPgTCBA00",
        "wCreVK_5ILlm",
        "ueIGy-SKjelB",
        "SFAVMDkTHxvq",
        "HrKHf2yMLuJm",
        "vcRXC8dfLuJx",
        "4zX_q1XiLuJy",
        "7LekN0CrLuJz",
        "riMjgZwPkFVe",
        "tWfTLjiBkFVf",
        "Kzn7tJ5TkFVg",
        "Sh2hXsq7kFVh",
        "ech4SnqwkdfJ",
        "lqi4NPX1kdfK",
        "3ot3-cxGkdfM",
        "48q1uvevkdfN",
        "C-hoFWzHkujA",
        "LkUdklGRkujD",
        "uQyLXaR0kujF",
        "sbl95JHCkujI",
        "d1KBYk0ik_mP",
        "P0wsp2e2k_mQ",
        "As_39jd_k_mS",
        "AzicBHXEk_mT",
        "jlQx5WW7lRIM",
        "XKK5PsYPlRIN",
        "shDs7ADmlRIO",
        "IC9Z4sWolRIQ",
        "i-uTgdF7MlSA",
        "kozO9vemMlSD",
        "TwTn5mFPMlSF",
        "epNUyLUjHhRA",
        "IGO8vNN4ljHv",
        "JsUvvKJVljHx",
        "VL5J6hLdJDWF",
        "46HxE5lDQ48U",
        "1griK4gNvZNF",
        "kd-_ObZEvEPM",
        "sGe8mvCBljIA",
        "aHrCVCr80yy-",
        "pvufj8qCvfrI",
        "PZeyA5qJ56XZ",
        "uarDIbgMpgkz",
        "ebMmyGhcpgk1",
        "ATIzS1LRpgk4",
        "ziEx9lIUpgk6",
        "C6U1s7RaMdzk"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}